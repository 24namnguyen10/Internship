import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings # <-- THAY Äá»”I
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- Háº±ng sá»‘ ---
DB_DIR = "db_chroma"
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2" # <-- Model local

# --- Prompt Template (Giá»¯ nguyÃªn) ---
RAG_PROMPT_TEMPLATE = """
Dá»±a CHÃNH XÃC vÃ o ná»™i dung Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
KHÃ”NG sá»­ dá»¥ng báº¥t ká»³ kiáº¿n thá»©c nÃ o khÃ¡c bÃªn ngoÃ i.
Náº¿u ná»™i dung khÃ´ng chá»©a cÃ¢u tráº£ lá»i, hÃ£y nÃ³i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin tráº£ lá»i trong tÃ i liá»‡u."

Ná»™i dung:
{context}

CÃ¢u há»i:
{question}

CÃ¢u tráº£ lá»i (chá»‰ dá»±a vÃ o ná»™i dung trÃªn):
"""

def main():
    # 1. Táº£i API Key (Váº«n cáº§n cho LLM chat)
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("Lá»—i: KhÃ´ng tÃ¬m tháº¥y GOOGLE_API_KEY.")
        return

    # 2. Kiá»ƒm tra thÆ° má»¥c DB
    if not os.path.exists(DB_DIR):
        print(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c vector store '{DB_DIR}'.")
        print("Vui lÃ²ng cháº¡y file 'ingest.py' trÆ°á»›c Ä‘á»ƒ táº¡o database.")
        return

    # 3. Táº£i (Load) Embedding Model (Local) <-- THAY Äá»”I
    print(f"Äang táº£i embedding model: {EMBED_MODEL}...")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL,
        model_kwargs={'device': 'cpu'}
    )

    # 4. Táº£i (Load) ChromaDB Ä‘Ã£ tá»“n táº¡i
    print(f"Äang táº£i vector store tá»« thÆ° má»¥c: {DB_DIR}...")
    db = Chroma(
        persist_directory=DB_DIR, 
        embedding_function=embeddings # <-- Sá»­ dá»¥ng embedding local
    )

    # 5. Táº¡o Retriever
    retriever = db.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )

    # 6. Táº£i (Load) LLM (MÃ´ hÃ¬nh Gemini)
    print("Äang khá»Ÿi táº¡o mÃ´ hÃ¬nh ngÃ´n ngá»¯ (LLM)...")
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash", # Giá»¯ nguyÃªn model chat
        google_api_key=api_key
    )

    # 7. Táº¡o RAG Chain (Giá»¯ nguyÃªn)
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

    rag_chain = (
        {"context": lambda x: format_docs(retriever.invoke(x["question"])), "question": lambda x: x["question"]}
        | rag_prompt
        | llm
        | StrOutputParser()
    )

    # 8. Báº¯t Ä‘áº§u vÃ²ng láº·p há»i-Ä‘Ã¡p
    print("\n--- ðŸ¤– Bot Ä‘Ã£ sáºµn sÃ ng! ---")
    print("Nháº­p cÃ¢u há»i cá»§a báº¡n (hoáº·c gÃµ 'exit' Ä‘á»ƒ thoÃ¡t).")

    while True:
        question_text = input("\nBáº¡n há»i: ")
        if question_text.lower() == 'exit':
            break
        
        print("Bot Ä‘ang tÃ¬m cÃ¢u tráº£ lá»i...")
        
        try:
            chain_input = {"question": question_text}
            answer = rag_chain.invoke(chain_input)
            print(f"\nBot tráº£ lá»i:\n{answer}")
        except Exception as e:
            # Lá»—i 429 váº«n cÃ³ thá»ƒ xáº£y ra á»Ÿ Ä‘Ã¢y (cho LLM chat), nhÆ°ng Ã­t hÆ¡n
            print(f"ÄÃ£ xáº£y ra lá»—i khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n: {e}")

    print("Táº¡m biá»‡t!")

if __name__ == "__main__":
    main()